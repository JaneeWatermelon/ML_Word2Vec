# Word2Vec (Skip-gram) with Negative Sampling and Numba Acceleration

Данный проект содержит реализацию алгоритма Word2Vec (модель Skip-gram) с использованием negative sampling и ускорением вычислений через Numba (JIT-компиляция).
Проект выполнен в учебных и исследовательских целях и демонстрирует реализацию Word2Vec с нуля, без использования готовых библиотек (например, gensim).

Также я описал [ноутбук в Google Colab](https://colab.research.google.com/drive/1UEJtnnV6mz8xuEucbhZWfIGSLWASgDgt?usp=sharing), где можно самому "позапускать" обучение, а также более подробно ознакомиться с теоретическим описанием, формулами и экспериментами.

## Основные возможности:

* Реализация Skip-gram Word2Vec
* Negative Sampling вместо полного softmax
* Взвешивание контекста по расстоянию
* JIT-компиляция с помощью Numba
* Параллельная обработка предложений
* Анализ семантической близости слов

## Используемые технологии:

* Python 3
* NumPy
* Numba
* scikit-learn

## Краткое описание алгоритма:

### Модель обучается по схеме Skip-gram:

* для каждого центрального слова предсказываются слова из его контекста в фиксированном окне.

### Для ускорения обучения используется negative sampling, при котором веса обновляются только для:

* положительной пары (центральное слово — контекст),
* и k отрицательных примеров, случайно выбранных из словаря.

### Для повышения производительности:

* основной цикл обучения JIT-компилируется,
* используется параллельная обработка предложений,
* вычисления выполняются напрямую над NumPy-массивами.

## Обучение модели

### Пример запуска обучения:

```
main(
    dimen=100,
    alpha=0.05,
    window_size=2,
    k=3,
    n_epochs=5
)
```

### Параметры:

* `dimen` — размерность эмбеддингов
* `alpha` — скорость обучения
* `window_size` — размер контекстного окна
* `k` — число негативных примеров
* `n_epochs` — количество эпох

## Анализ результатов

### После обучения:

* эмбеддинги сохраняются в файл `v_u_vocs.npz`,
* словарь сохраняется в `voc.txt`.

### Реализован анализ:

* косинусного сходства между словами,
* поиск семантически близких слов,
